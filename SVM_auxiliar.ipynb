{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto ML\n",
    "\n",
    "Trabalho de:\n",
    "\n",
    "* Bárbara Simões Neto up\n",
    "* Beatriz Castro up\n",
    "* Rodrigo Couto up202104696\n",
    "\n",
    "\n",
    "sites usados até então:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html\n",
    "\n",
    "https://link.springer.com/article/10.1007/s10618-021-00737-9 (so vi as imagens para perceber o que era os diferentes tipos de datasets que o chatzaome deu, elas sao bastantes intuitivas)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, make_moons, make_circles, make_blobs\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Gerar datasets\n",
    "\n",
    "def generate_mixed_dataset(n_samples, n_features, n_categorical, n_ordinal, n_integer, \n",
    "                           n_classes, class_balance=None, noise=0.0, \n",
    "                           dataset_type='linear', random_state=42):\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    if dataset_type == 'linear':\n",
    "        X, y = make_classification(n_samples=n_samples, n_features=n_features, \n",
    "                                  n_informative=n_features, n_redundant=0, \n",
    "                                  n_classes=n_classes, weights=class_balance, \n",
    "                                  flip_y=noise, random_state=random_state, class_sep=2)\n",
    "    elif dataset_type == 'moons':\n",
    "        X, y = make_moons(n_samples=n_samples, noise=noise, random_state=random_state)\n",
    "        if n_features > 2:\n",
    "            extra_features = np.random.rand(n_samples, n_features - 2) * 100\n",
    "            X = np.hstack([X, extra_features])\n",
    "    elif dataset_type == 'circles':\n",
    "        X, y = make_circles(n_samples=n_samples, noise=noise, random_state=random_state)\n",
    "        if n_features > 2:\n",
    "            extra_features = np.random.rand(n_samples, n_features - 2) * 100\n",
    "            X = np.hstack([X, extra_features])\n",
    "    elif dataset_type == 'blobs':\n",
    "        X, y = make_blobs(n_samples=n_samples, centers=n_classes, cluster_std=noise*10, \n",
    "                          random_state=random_state)\n",
    "        if n_features > 2:\n",
    "            extra_features = np.random.rand(n_samples, n_features - 2) * 100\n",
    "            X = np.hstack([X, extra_features])\n",
    "    else:\n",
    "        raise ValueError(\"Tipo de dataset não suportado. Use 'linear', 'moons', 'circles' ou 'blobs'.\")\n",
    "\n",
    "    categorical_data = np.random.choice(['A', 'B', 'C', 'D', 'E'], size=(n_samples, n_categorical))\n",
    "\n",
    "    ordinal_data = np.random.randint(1, 6, size=(n_samples, n_ordinal))  #podem mudar estes valores para criar dataset mais especificos\n",
    "\n",
    "    integer_data = np.random.randint(0, 100, size=(n_samples, n_integer))  #podem mudar estes valores para criar dataset mais especificos\n",
    "\n",
    "    # Combina todos os dados em um único array\n",
    "    data = np.hstack([X, categorical_data, ordinal_data, integer_data])\n",
    "\n",
    "    columns = [f'Continuous_{i+1}' for i in range(n_features)] + \\\n",
    "              [f'Categorical_{i+1}' for i in range(n_categorical)] + \\\n",
    "              [f'Ordinal_{i+1}' for i in range(n_ordinal)] + \\\n",
    "              [f'Integer_{i+1}' for i in range(n_integer)]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    # Adiciona a coluna target\n",
    "    df['Target'] = y\n",
    "\n",
    "    # Converte as colunas para os tipos corretos\n",
    "    for col in df.columns:\n",
    "        if col.startswith('Continuous'):\n",
    "            df[col] = df[col].astype(float)\n",
    "        elif col.startswith('Ordinal') or col.startswith('Integer'):\n",
    "            df[col] = df[col].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVMs work best with specific types of datasets. They are ideal for linearly separable data, where classes can be clearly divided by a straight line. They also perform well with high-dimensional data, like text or datasets with many features, as they focus on finding a linear decision boundary. Numerical data (continuous or integer values) works best, while categorical data needs to be transformed (e.g., using one-hot encoding). However, linear SVMs have limitations. They struggle with large datasets due to high computational costs and memory usage. They also perform poorly on imbalanced datasets, often favoring the majority class, and on noisy data where classes overlap significantly. For multiclass problems, additional techniques like One-vs-Rest or One-vs-One are needed, as linear SVMs are designed for binary classification. In summary, linear SVMs are best suited for small to medium-sized, high-dimensional, linearly separable datasets with balanced classes and minimal noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Dataset for Linear SVM:\n",
      "   Continuous_1  Continuous_2  Continuous_3  Continuous_4  Continuous_5  \\\n",
      "0     -2.307608      1.668358      2.252106     -1.968564     -4.929186   \n",
      "1      4.389963     -0.578385     -2.722650     -2.982947      7.632946   \n",
      "2     -1.897597      2.994106      0.755542     -5.697095      2.729707   \n",
      "3      3.867551     -1.917197      2.019131     -0.850470      2.443705   \n",
      "4     -2.549646      2.202160      2.300397      0.795324     -3.497897   \n",
      "\n",
      "   Continuous_6  Continuous_7  Continuous_8  Continuous_9  Continuous_10  ...  \\\n",
      "0      0.772611     -5.188637      0.909699      0.951789       1.001922  ...   \n",
      "1      3.088414     -2.569981      3.337391     -6.984058      -0.624044  ...   \n",
      "2     -0.131957     -1.698201      0.369110     -0.722678      -1.358863  ...   \n",
      "3      3.185501     -4.957228     -4.213375     -0.848798       1.247323  ...   \n",
      "4     -2.680108     -2.706487      5.675596      2.201553      -4.804617  ...   \n",
      "\n",
      "   Continuous_20  Integer_1  Integer_2  Integer_3  Integer_4  Integer_5  \\\n",
      "0       0.922478         51         92         14         71         60   \n",
      "1      -6.481013         74         74         87         99         23   \n",
      "2       2.388573          1         87         29         37          1   \n",
      "3      -3.524633         32         75         57         21         88   \n",
      "4       0.128734         41         91         59         79         14   \n",
      "\n",
      "   Integer_6  Integer_7  Integer_8  Target  \n",
      "0         20         82         86       1  \n",
      "1          2         21         52       1  \n",
      "2         63         59         20       1  \n",
      "3         48         90         58       1  \n",
      "4         61         61         46       1  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "#gerar dataset adequado\n",
    "\n",
    "df_svm_linear = generate_mixed_dataset(n_samples=500, n_features=20, n_categorical=0, n_ordinal=0, n_integer=8,\n",
    "                                                  n_classes=2, class_balance=None, noise=0.1, dataset_type='linear')\n",
    "\n",
    "print(\"Best Dataset for Linear SVM:\")\n",
    "print(df_svm_linear.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 0.83 (± 0.04)\n",
      "\n",
      "Fold Accuracies:\n",
      "Fold 1: 0.80\n",
      "Fold 2: 0.83\n",
      "Fold 3: 0.83\n",
      "Fold 4: 0.80\n",
      "Fold 5: 0.90\n"
     ]
    }
   ],
   "source": [
    "X = df_svm_linear.drop('Target', axis=1)  \n",
    "y = df_svm_linear['Target']  \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# standerizar as features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Training model\n",
    "model = LinearSVC(C=0.1,random_state=42)  \n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#Predicts\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#novo dataset para avaliar o modelo\n",
    "df_svm_linear_2 = generate_mixed_dataset(n_samples=150, n_features=20, n_categorical=0, n_ordinal=0, n_integer=8,\n",
    "                                           n_classes=2, class_balance=None, noise=0.1, dataset_type='linear')\n",
    "\n",
    "\n",
    "\n",
    "X_2 = df_svm_linear_2.drop('Target', axis=1)\n",
    "y_2 = df_svm_linear_2['Target']\n",
    "\n",
    "\n",
    "X_2 = scaler.transform(X_2)\n",
    "\n",
    "y_pred_2 = model.predict(X_2)\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# cross-validation \n",
    "cv_scores_2 = cross_val_score(model, X_2, y_2, cv=5, scoring='accuracy')\n",
    "\n",
    "# Resultados\n",
    "print(f\"Cross-Validation Accuracy: {cv_scores_2.mean():.2f} (± {cv_scores_2.std():.2f})\")\n",
    "print(\"\\nFold Accuracies:\")\n",
    "for i, score in enumerate(cv_scores_2, start=1):\n",
    "    print(f\"Fold {i}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM RBF (Radial Basis Function) kernel is ideal for non-linear and complex datasets, as it can capture intricate patterns where classes are not linearly separable. It also handles noisy or overlapping data better than linear SVMs, creating flexible decision boundaries. However, its performance depends heavily on hyperparameter tuning, such as the gamma value. For other aspects, like handling high-dimensional data, numerical features, large datasets, imbalanced classes, and multiclass problems, it behaves similarly to linear SVMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhor Dataset para SVM RBF:\n",
      "   Continuous_1  Continuous_2  Continuous_3  Continuous_4  Continuous_5  \\\n",
      "0      0.830858     -0.334342     37.454012     95.071431     73.199394   \n",
      "1      0.991710      0.879000     43.194502     29.122914     61.185289   \n",
      "2      1.107245     -0.470344     30.461377      9.767211     68.423303   \n",
      "3     -0.140899      1.033148     59.789998     92.187424      8.849250   \n",
      "4      0.405592      1.328529      0.552212     81.546143     70.685734   \n",
      "\n",
      "   Continuous_6  Continuous_7  Continuous_8  Continuous_9  Continuous_10  ...  \\\n",
      "0     59.865848     15.601864     15.599452      5.808361      86.617615  ...   \n",
      "1     13.949386     29.214465     36.636184     45.606998      78.517596  ...   \n",
      "2     44.015249     12.203823     49.517691      3.438852      90.932040  ...   \n",
      "3     19.598286      4.522729     32.533033     38.867729      27.134903  ...   \n",
      "4     72.900717     77.127035      7.404465     35.846573      11.586906  ...   \n",
      "\n",
      "   Ordinal_1  Integer_1  Integer_2  Integer_3  Integer_4  Integer_5  \\\n",
      "0          5         88         53         56         19         48   \n",
      "1          2         92         12          2          1         15   \n",
      "2          4         70          4         55         70         97   \n",
      "3          5         29         24         84         59         58   \n",
      "4          1         19         33          4         81         59   \n",
      "\n",
      "   Integer_6  Integer_7  Integer_8  Target  \n",
      "0          6         28         47       1  \n",
      "1         93          5         44       0  \n",
      "2         31         48         12       1  \n",
      "3         84         27         57       0  \n",
      "4         42         56         56       0  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "#gerar dataset\n",
    "df_svm_rbf = generate_mixed_dataset(n_samples=500, n_features=20, n_categorical=0,n_ordinal=1,n_integer=8,\n",
    "    n_classes=2,class_balance=None,noise=0.2,dataset_type='moons', random_state=42)\n",
    "\n",
    "print(\"\\nMelhor Dataset para SVM RBF:\")\n",
    "print(df_svm_rbf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 0.88 (± 0.04)\n",
      "\n",
      "Fold Accuracies:\n",
      "Fold 1: 0.90\n",
      "Fold 2: 0.80\n",
      "Fold 3: 0.90\n",
      "Fold 4: 0.90\n",
      "Fold 5: 0.88\n"
     ]
    }
   ],
   "source": [
    "X = df_svm_rbf.drop('Target', axis=1)  \n",
    "y = df_svm_rbf['Target']  \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# standerizar as features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Training model\n",
    "model = SVC(C=1.0, kernel='rbf', gamma='scale', random_state=42) \n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicts\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# novo dataset para avaliar o modelo\n",
    "df_svm_rbf_2 = generate_mixed_dataset(n_samples=200, n_features=20, n_categorical=0, n_ordinal=1, n_integer=8,\n",
    "                                     n_classes=2, class_balance=None, noise=0.2, dataset_type='moons')\n",
    "\n",
    "X_2 = df_svm_rbf_2.drop('Target', axis=1)\n",
    "y_2 = df_svm_rbf_2['Target']\n",
    "\n",
    "X_2 = scaler.transform(X_2)\n",
    "\n",
    "y_pred_2 = model.predict(X_2)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# cross-validation \n",
    "cv_scores_2 = cross_val_score(model, X_2, y_2, cv=5, scoring='accuracy')\n",
    "\n",
    "# Resultados\n",
    "print(f\"Cross-Validation Accuracy: {cv_scores_2.mean():.2f} (± {cv_scores_2.std():.2f})\")\n",
    "print(\"\\nFold Accuracies:\")\n",
    "for i, score in enumerate(cv_scores_2, start=1):\n",
    "    print(f\"Fold {i}: {score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
